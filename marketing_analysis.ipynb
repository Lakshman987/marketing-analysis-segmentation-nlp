"""
Marketing Analysis: Customer Segmentation + NLP (No sqlite3/NLTK Dependencies)
--------------------------------------------------------------------------------
Fix: Replaced TextBlob/NLTK sentiment with a dependency-light implementation to avoid
`ModuleNotFoundError: No module named 'sqlite3'`. The notebook now:
  - Uses KMeans for segmentation
  - Uses a VADER-based sentiment analyzer if available; otherwise falls back to a
    tiny rule-based lexicon sentiment (no sqlite3/NLTK needed)
  - Performs LDA topic modeling
  - Adds lightweight tests to validate behavior

If you expect different behavior (e.g., a specific number of segments, a
particular sentiment scale, or different outputs/files), please tell me and I’ll
adjust accordingly.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from typing import List

# ------------------
# Sentiment (No sqlite3 / NLTK)
# ------------------
# Try VADER (lightweight, no sqlite3). If unavailable, use a tiny rule-based lexicon.
try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer  # type: ignore
    _VADER = SentimentIntensityAnalyzer()

    def sentiment_score(text: str) -> float:
        """Return VADER compound score in [-1, 1]."""
        if not isinstance(text, str):
            return 0.0
        return float(_VADER.polarity_scores(text).get("compound", 0.0))

    SENTIMENT_ENGINE = "vader"
except Exception:
    POS_WORDS = {
        "love", "great", "helpful", "excellent", "value", "happy", "good",
        "fast", "amazing", "quality", "satisfied"
    }
    NEG_WORDS = {
        "too", "expensive", "not", "late", "terrible", "won't", "wont",
        "confusing", "broke", "bad", "awful"
    }

    def _tokenize(text: str) -> List[str]:
        return [t.strip(".,!?:;’'\"()[]{}-").lower() for t in text.split()]

    def sentiment_score(text: str) -> float:
        """Simple lexicon-based score in [-1, 1] (no external deps)."""
        if not isinstance(text, str):
            return 0.0
        toks = _tokenize(text)
        pos = sum(t in POS_WORDS for t in toks)
        neg = sum(t in NEG_WORDS for t in toks)
        total = pos + neg
        if total == 0:
            return 0.0
        return (pos - neg) / total

    SENTIMENT_ENGINE = "lexicon"

# ------------------
# Load Sample Dataset (kept unchanged)
# ------------------
# Using a mock marketing dataset with customer features and reviews
data = {
    'CustomerID': [1,2,3,4,5,6,7,8,9,10],
    'Age': [25,34,45,23,35,52,46,51,62,44],
    'Annual_Income_k': [15,29,45,21,35,70,60,88,120,55],
    'Spending_Score': [39,81,6,77,40,76,94,3,14,73],
    'Review': [
        "Love the product, great quality!",
        "Too expensive for what it offers",
        "Customer service was very helpful",
        "Not satisfied, delivery was late",
        "Excellent value for money",
        "Terrible experience, won’t buy again",
        "Very happy with the purchase",
        "The website was confusing to use",
        "Product broke after one use",
        "Fast shipping and good quality"
    ]
}

df = pd.DataFrame(data)

# ------------------
# Customer Segmentation
# ------------------
features = df[['Age','Annual_Income_k','Spending_Score']]
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
df['Segment'] = kmeans.fit_predict(scaled_features)

# Plot Segmentation
plt.figure()
plt.scatter(df['Annual_Income_k'], df['Spending_Score'], c=df['Segment'])
plt.xlabel('Annual Income (k)')
plt.ylabel('Spending Score')
plt.title('Customer Segmentation')
plt.tight_layout()
plt.show()

# ------------------
# Sentiment Analysis on Reviews
# ------------------
df['Sentiment'] = df['Review'].apply(sentiment_score)

# Average sentiment per segment
segment_sentiment = df.groupby('Segment')['Sentiment'].mean().sort_index()
print(f"Sentiment engine: {SENTIMENT_ENGINE}")
print("Average Sentiment per Segment:\n", segment_sentiment)

# ------------------
# Topic Modeling (Reviews)
# ------------------
vectorizer = CountVectorizer(stop_words='english')
review_matrix = vectorizer.fit_transform(df['Review'])

lda = LatentDirichletAllocation(n_components=2, random_state=42)
lda.fit(review_matrix)

terms = vectorizer.get_feature_names_out()
for idx, topic in enumerate(lda.components_):
    top_idx = topic.argsort()[-5:]
    print(f"Topic {idx} top terms:", [terms[i] for i in top_idx])

# Save cleaned dataset with segmentation + sentiment
out_path = "customer_segmentation_sentiment.csv"
df.to_csv(out_path, index=False)
print(f"\nData with segmentation and sentiment saved to '{out_path}'")

# ------------------
# Lightweight Tests
# ------------------
# These help ensure the notebook works as intended. They do not change behavior.

def run_tests():
    # Sentiment tests
    pos_text = "I love this, it's amazing and great quality!"
    neg_text = "This is terrible, confusing, and bad."
    neutral_text = "The product exists."

    s_pos = sentiment_score(pos_text)
    s_neg = sentiment_score(neg_text)
    s_neu = sentiment_score(neutral_text)

    assert s_pos > s_neg, "Positive sentiment should score higher than negative."
    # Ensure at least one of the polarity directions behaves as expected
    assert (s_pos > 0.0) or (s_neg < 0.0), "Expected positive or negative polarity to be reflected."

    # Segmentation tests
    assert df['Segment'].nunique() == 3, "Expected exactly 3 customer segments."
    assert len(df) == 10, "Dataset size should remain 10."

    # Topic model shape
    assert lda.components_.shape[0] == 2, "LDA should produce 2 topics."

    print("\nAll lightweight tests passed.")

run_tests()
